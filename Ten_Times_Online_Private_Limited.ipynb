{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "8yIaqA7F6S4N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install necessary libraries\n",
        "!pip install feedparser nltk sqlalchemy pandas scikit-learn\n",
        "\n",
        "# Import libraries\n",
        "import feedparser\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import string\n",
        "from nltk.corpus import stopwords\n",
        "import sqlite3\n",
        "import os\n",
        "\n",
        "# Download NLTK data\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Preprocessing function\n",
        "def preprocess_text(text):\n",
        "    text = text.lower()  # Convert to lowercase\n",
        "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))  # Remove punctuation\n",
        "    tokens = nltk.word_tokenize(text)  # Tokenize the text\n",
        "    tokens = [word for word in tokens if word not in stopwords.words('english')]  # Remove stopwords\n",
        "    return \" \".join(tokens)\n",
        "\n",
        "# Feed parsing and data extraction\n",
        "def parse_feeds(feeds):\n",
        "    articles = []\n",
        "    for feed in feeds:\n",
        "        try:\n",
        "            parsed_feed = feedparser.parse(feed)\n",
        "            for entry in parsed_feed.entries:\n",
        "                content = entry.summary if 'summary' in entry else entry.description if 'description' in entry else ''\n",
        "                processed_content = preprocess_text(content)\n",
        "                articles.append({\n",
        "                    'title': entry.title,\n",
        "                    'content': processed_content,\n",
        "                    'published': entry.published if 'published' in entry else 'N/A',\n",
        "                    'link': entry.link\n",
        "                })\n",
        "        except Exception as e:\n",
        "            print(f\"Error parsing feed {feed}: {e}\")\n",
        "    return articles\n",
        "\n",
        "# Improved categorization function with expanded keywords\n",
        "def categorize_article(article):\n",
        "    terrorism_keywords = ['terrorism', 'protest', 'political unrest', 'riot', 'bomb', 'attack', 'violence', 'terrorist', 'extremist']\n",
        "    positive_keywords = ['positive', 'uplifting', 'success', 'achievement', 'award', 'celebration', 'milestone', 'good news', 'happy']\n",
        "    disaster_keywords = ['disaster', 'earthquake', 'flood', 'hurricane', 'natural disaster', 'tsunami', 'wildfire', 'storm', 'storming', 'damage']\n",
        "\n",
        "    # Check for category based on keywords\n",
        "    if any(keyword in article for keyword in terrorism_keywords):\n",
        "        return 'Terrorism / protest / political unrest / riot'\n",
        "    elif any(keyword in article for keyword in positive_keywords):\n",
        "        return 'Positive/Uplifting'\n",
        "    elif any(keyword in article for keyword in disaster_keywords):\n",
        "        return 'Natural Disasters'\n",
        "    else:\n",
        "        return 'Others'\n",
        "\n",
        "# Define RSS feeds\n",
        "rss_feeds = [\n",
        "    'http://rss.cnn.com/rss/cnn_topstories.rss',\n",
        "    'http://qz.com/feed',\n",
        "    'http://feeds.foxnews.com/foxnews/politics',\n",
        "    'http://feeds.reuters.com/reuters/businessNews',\n",
        "    'http://feeds.feedburner.com/NewshourWorld',\n",
        "    'https://feeds.bbci.co.uk/news/world/asia/india/rss.xml'\n",
        "]\n",
        "\n",
        "# Parse feeds and categorize articles\n",
        "articles = parse_feeds(rss_feeds)\n",
        "\n",
        "# Create a DataFrame\n",
        "df = pd.DataFrame(articles)\n",
        "df['category'] = df['content'].apply(categorize_article)\n",
        "\n",
        "# Check category distribution\n",
        "print(\"Initial Category Distribution:\")\n",
        "print(df['category'].value_counts())\n",
        "\n",
        "# Setup SQLite database\n",
        "db_path = 'news_articles.db'\n",
        "if os.path.exists(db_path):\n",
        "    os.remove(db_path)  # Remove existing database for fresh start\n",
        "\n",
        "conn = sqlite3.connect(db_path)\n",
        "cursor = conn.cursor()\n",
        "\n",
        "# Create table to store news articles\n",
        "cursor.execute('''\n",
        "    CREATE TABLE IF NOT EXISTS news_articles (\n",
        "        id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "        title TEXT UNIQUE,\n",
        "        content TEXT,\n",
        "        published TEXT,\n",
        "        link TEXT,\n",
        "        category TEXT\n",
        "    )\n",
        "''')\n",
        "conn.commit()\n",
        "\n",
        "# Function to insert articles into database, avoiding duplicates\n",
        "def save_article_to_db(title, content, published, link, category):\n",
        "    try:\n",
        "        cursor.execute('''\n",
        "            INSERT INTO news_articles (title, content, published, link, category)\n",
        "            VALUES (?, ?, ?, ?, ?)\n",
        "        ''', (title, content, published, link, category))\n",
        "        conn.commit()\n",
        "    except sqlite3.IntegrityError:\n",
        "        pass  # Duplicate entry, ignore\n",
        "\n",
        "# Save articles to database\n",
        "for index, row in df.iterrows():\n",
        "    save_article_to_db(row['title'], row['content'], row['published'], row['link'], row['category'])\n",
        "\n",
        "# Fetch data from database\n",
        "cursor.execute(\"SELECT title, content, category FROM news_articles\")\n",
        "rows = cursor.fetchall()\n",
        "\n",
        "# Create a new DataFrame from database\n",
        "df_db = pd.DataFrame(rows, columns=['title', 'content', 'category'])\n",
        "\n",
        "# Check category distribution after database fetch\n",
        "print(\"\\nDatabase Category Distribution:\")\n",
        "print(df_db['category'].value_counts())\n",
        "\n",
        "# Check if there are enough data points for training\n",
        "if df_db['category'].nunique() < 2:\n",
        "    print(\"Not enough categories for training a classifier.\")\n",
        "else:\n",
        "    # Check class distribution\n",
        "    print(\"\\nClass Distribution:\")\n",
        "    print(df_db['category'].value_counts())\n",
        "\n",
        "    # Handle class imbalance by using class weights in the classifier\n",
        "    X = df_db['content']\n",
        "    y = df_db['category']\n",
        "\n",
        "    # Splitting the data for training and testing with stratification\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
        "\n",
        "    # Vectorize the text data with bi-grams\n",
        "    vectorizer = TfidfVectorizer(ngram_range=(1,2), max_features=5000)\n",
        "    X_train_vec = vectorizer.fit_transform(X_train)\n",
        "    X_test_vec = vectorizer.transform(X_test)\n",
        "\n",
        "    # Train a Logistic Regression classifier with class weights\n",
        "    model = LogisticRegression(max_iter=1000, class_weight='balanced')\n",
        "    model.fit(X_train_vec, y_train)\n",
        "\n",
        "    # Predicting categories for the test set\n",
        "    y_pred = model.predict(X_test_vec)\n",
        "\n",
        "    # Evaluate the model\n",
        "    print(f\"\\nAccuracy: {accuracy_score(y_test, y_pred)}\")\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(classification_report(y_test, y_pred))\n",
        "\n",
        "    # Apply the model to all data\n",
        "    df_db['predicted_category'] = model.predict(vectorizer.transform(df_db['content']))\n",
        "\n",
        "    # Save the categorized articles to a CSV file\n",
        "    df_db.to_csv('classified_news_articles.csv', index=False)\n",
        "    print(\"\\nNews articles categorized and saved to 'classified_news_articles.csv'\")\n",
        "\n",
        "# Display the categorized DataFrame\n",
        "df_categorized = pd.read_csv('classified_news_articles.csv')\n",
        "print(\"\\nSample of Categorized Articles:\")\n",
        "print(df_categorized.head())\n",
        "\n",
        "# Download the CSV file (only works in Google Colab)\n",
        "from google.colab import files\n",
        "files.download('classified_news_articles.csv')\n",
        "\n",
        "# Close the database connection\n",
        "conn.close()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ZkgBjmnQ6TPY",
        "outputId": "5568465b-4431-4bc6-9889-a0d06c82f88d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: feedparser in /usr/local/lib/python3.10/dist-packages (6.0.11)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: sqlalchemy in /usr/local/lib/python3.10/dist-packages (2.0.35)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.5.2)\n",
            "Requirement already satisfied: sgmllib3k in /usr/local/lib/python3.10/dist-packages (from feedparser) (1.0.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy) (4.12.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy) (3.1.1)\n",
            "Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial Category Distribution:\n",
            "category\n",
            "Others                                           175\n",
            "Natural Disasters                                  9\n",
            "Positive/Uplifting                                 6\n",
            "Terrorism / protest / political unrest / riot      5\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Database Category Distribution:\n",
            "category\n",
            "Others                                           175\n",
            "Natural Disasters                                  9\n",
            "Positive/Uplifting                                 6\n",
            "Terrorism / protest / political unrest / riot      5\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Class Distribution:\n",
            "category\n",
            "Others                                           175\n",
            "Natural Disasters                                  9\n",
            "Positive/Uplifting                                 6\n",
            "Terrorism / protest / political unrest / riot      5\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Accuracy: 0.8813559322033898\n",
            "\n",
            "Classification Report:\n",
            "                                               precision    recall  f1-score   support\n",
            "\n",
            "                            Natural Disasters       0.33      0.33      0.33         3\n",
            "                                       Others       0.91      0.96      0.94        53\n",
            "                           Positive/Uplifting       0.00      0.00      0.00         2\n",
            "Terrorism / protest / political unrest / riot       0.00      0.00      0.00         1\n",
            "\n",
            "                                     accuracy                           0.88        59\n",
            "                                    macro avg       0.31      0.32      0.32        59\n",
            "                                 weighted avg       0.84      0.88      0.86        59\n",
            "\n",
            "\n",
            "News articles categorized and saved to 'classified_news_articles.csv'\n",
            "\n",
            "Sample of Categorized Articles:\n",
            "                                               title  \\\n",
            "0  Some on-air claims about Dominion Voting Syste...   \n",
            "1  Dominion still has pending lawsuits against el...   \n",
            "2  Here are the 20 specific Fox broadcasts and tw...   \n",
            "3  Judge in Fox News-Dominion defamation trial: '...   \n",
            "4  'Difficult to say with a straight face': Tappe...   \n",
            "\n",
            "                                             content category  \\\n",
            "0                                                NaN   Others   \n",
            "1                                                NaN   Others   \n",
            "2  • foxdominion trial delay unusual judge says •...   Others   \n",
            "3  judge announced court settlement reached histo...   Others   \n",
            "4  settlement reached dominion voting systems def...   Others   \n",
            "\n",
            "  predicted_category  \n",
            "0             Others  \n",
            "1             Others  \n",
            "2             Others  \n",
            "3             Others  \n",
            "4             Others  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_4f694a57-3b86-4347-9ad3-a25ccd704927\", \"classified_news_articles.csv\", 61568)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}